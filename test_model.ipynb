{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "from torchviz import make_dot\n",
    "from torch.cuda.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size, stride=1, padding=padding)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "        self.downsample = downsample  # For matching dimensions if needed\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "    \n",
    "# Utility Function\n",
    "def crop_to_match(tensor, target_tensor):\n",
    "    \"\"\"\n",
    "    Crops tensor to match the size of target_tensor along spatial dimensions (D, H, W).\n",
    "    \"\"\"\n",
    "    diff_depth = tensor.size(2) - target_tensor.size(2)\n",
    "    diff_height = tensor.size(3) - target_tensor.size(3)\n",
    "    diff_width = tensor.size(4) - target_tensor.size(4)\n",
    "\n",
    "    # Crop along each dimension\n",
    "    tensor = tensor[:, :, \n",
    "                    diff_depth // 2:tensor.size(2) - diff_depth // 2,\n",
    "                    diff_height // 2:tensor.size(3) - diff_height // 2,\n",
    "                    diff_width // 2:tensor.size(4) - diff_width // 2]\n",
    "    return tensor\n",
    "\n",
    "# Cascade3DUNet Class\n",
    "class Cascade3DUNet(nn.Module):\n",
    "    def __init__(self, in_channels=2, num_classes=1):\n",
    "        super(Cascade3DUNet, self).__init__()\n",
    "\n",
    "        # Input projection to match encoder1's input size\n",
    "        self.input_projection = nn.Conv3d(in_channels, 32, kernel_size=1)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder1 = ResidualBlock(32, 32)\n",
    "        self.encoder2 = ResidualBlock(32, 64, downsample=nn.Conv3d(32, 64, kernel_size=1))\n",
    "        self.encoder3 = ResidualBlock(64, 128, downsample=nn.Conv3d(64, 128, kernel_size=1))\n",
    "        self.encoder4 = ResidualBlock(128, 256, downsample=nn.Conv3d(128, 256, kernel_size=1))\n",
    "        self.encoder5 = ResidualBlock(256, 512, downsample=nn.Conv3d(256, 512, kernel_size=1))\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder4 = nn.ConvTranspose3d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder4_conv = nn.Conv3d(256, 256, kernel_size=3, padding=1)\n",
    "        self.decoder3 = nn.ConvTranspose3d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder3_conv = nn.Conv3d(128, 128, kernel_size=3, padding=1)\n",
    "        self.decoder2 = nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder2_conv = nn.Conv3d(64, 64, kernel_size=3, padding=1)\n",
    "        self.decoder1 = nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2)\n",
    "        self.decoder1_conv = nn.Conv3d(32, 32, kernel_size=3, padding=1)\n",
    "\n",
    "        # Projection layers for skip connections\n",
    "        self.proj4 = nn.Conv3d(256, 256, kernel_size=1)\n",
    "        self.proj3 = nn.Conv3d(128, 128, kernel_size=1)\n",
    "        self.proj2 = nn.Conv3d(64, 64, kernel_size=1)\n",
    "        self.proj1 = nn.Conv3d(32, 32, kernel_size=1)\n",
    "\n",
    "        # Final layer\n",
    "        self.final_conv = nn.Conv3d(32, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input projection\n",
    "        x = self.input_projection(x)  # Convert in_channels=2 to out_channels=32\n",
    "\n",
    "        # Encoder\n",
    "        enc1 = self.encoder1(x)  # [B, 32, D, H, W]\n",
    "        enc2 = self.encoder2(F.max_pool3d(enc1, 2))  # [B, 64, D/2, H/2, W/2]\n",
    "        enc3 = self.encoder3(F.max_pool3d(enc2, 2))  # [B, 128, D/4, H/4, W/4]\n",
    "        enc4 = self.encoder4(F.max_pool3d(enc3, 2))  # [B, 256, D/8, H/8, W/8]\n",
    "        enc5 = self.encoder5(F.max_pool3d(enc4, 2))  # [B, 512, D/16, H/16, W/16]\n",
    "\n",
    "        # Decoder\n",
    "        x = self.decoder4(enc5)  # [B, 256, D/8, H/8, W/8]\n",
    "        x = self.decoder4_conv(x)\n",
    "        x = crop_to_match(x, enc4) + self.proj4(enc4)\n",
    "\n",
    "        x = self.decoder3(x)  # [B, 128, D/4, H/4, W/4]\n",
    "        x = self.decoder3_conv(x)\n",
    "        x = crop_to_match(x, enc3) + self.proj3(enc3)\n",
    "\n",
    "        x = self.decoder2(x)  # [B, 64, D/2, H/2, W/2]\n",
    "        x = self.decoder2_conv(x)\n",
    "        x = crop_to_match(x, enc2) + self.proj2(enc2)\n",
    "\n",
    "        x = self.decoder1(x)  # [B, 32, D, H, W]\n",
    "        x = self.decoder1_conv(x)\n",
    "        x = crop_to_match(x, enc1) + self.proj1(enc1)\n",
    "\n",
    "        x = self.final_conv(x)  # Final layer\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Evaluation\n",
    "# Denormalize Dose\n",
    "\n",
    "def denormalize_dose(normalized_dose, min_dose, dynamic_max):\n",
    "    return normalized_dose * (dynamic_max - min_dose) + min_dose\n",
    "\n",
    "def predict_and_save_results(model, test_loader, save_path, dose_save_path, device=\"cuda\", metadata=None):\n",
    "    model.eval()\n",
    "    mae_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, inputs in enumerate(tqdm(test_loader, desc=\"Predicting\")):\n",
    "            ct = inputs[0, 0, :, :, :].cpu().numpy()\n",
    "            structure = inputs[0, 1, :, :, :].cpu().numpy()\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            # ทำนายผล\n",
    "            outputs = model(inputs)\n",
    "            prediction = outputs.squeeze(0).cpu().numpy()\n",
    "\n",
    "            # Denormalize predictions\n",
    "            patient_id = f\"patient_{i}\"\n",
    "            min_dose = metadata[patient_id][\"MinDose\"]\n",
    "            dynamic_max = metadata[patient_id][\"DynamicMaxDose\"]\n",
    "            pred_denorm = denormalize_dose(prediction, min_dose, dynamic_max)\n",
    "\n",
    "            # Save the predicted dose as .npy\n",
    "            np.save(os.path.join(dose_save_path, f'{patient_id}_predicted_dose.npy'), pred_denorm)\n",
    "\n",
    "            # Load Ground Truth\n",
    "            ground_truth = np.load(os.path.join(save_path, f\"{patient_id}_ground_truth.npy\"))\n",
    "            # Denormalize Ground Truth (GT) if it is normalized\n",
    "            ground_truth_denorm = denormalize_dose(ground_truth, min_dose, dynamic_max)\n",
    "            mae = np.mean(np.abs(pred_denorm - ground_truth_denorm))\n",
    "            mae_list.append(mae)\n",
    "\n",
    "            save_path_comparison = os.path.join(save_path, f\"{patient_id}_comparison.png\")\n",
    "            visualize_predictions(ct, structure, pred_denorm, ground_truth, save_path_comparison)\n",
    "\n",
    "    mae_mean = np.mean(mae_list)\n",
    "    mae_std = np.std(mae_list)\n",
    "\n",
    "    # Save results to CSV\n",
    "    results_df = pd.DataFrame({\"Patient\": [f\"patient_{i}\" for i in range(len(mae_list))], \"MAE\": mae_list})\n",
    "    results_df.to_csv(os.path.join(save_path, \"metrics.csv\"), index=False)\n",
    "    print(f\"Metrics saved to {os.path.join(save_path, 'metrics.csv')}\")\n",
    "    print(f\"Mean MAE: {mae_mean:.4f} Gy, Std Dev: {mae_std:.4f} Gy\")\n",
    "\n",
    "# Visualize Predictions\n",
    "def visualize_predictions(ct, structure, prediction, ground_truth=None, slice_indices=None, save_path=None):\n",
    "    if slice_indices is None:\n",
    "        slice_indices = [16, 32, 48]\n",
    "\n",
    "    n_cols = 3 if ground_truth is None else 4\n",
    "    fig, axes = plt.subplots(len(slice_indices), n_cols, figsize=(15, 5 * len(slice_indices)))\n",
    "\n",
    "    for i, slice_idx in enumerate(slice_indices):\n",
    "        axes[i, 0].imshow(ct[slice_idx, :, :], cmap='gray')\n",
    "        axes[i, 0].set_title(f'CT (Slice {slice_idx})')\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        axes[i, 1].imshow(structure[slice_idx, :, :], cmap='gray')\n",
    "        axes[i, 1].set_title(f'Structure (Slice {slice_idx})')\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "        axes[i, 2].imshow(prediction[slice_idx, :, :], cmap='hot')\n",
    "        axes[i, 2].set_title(f'Predicted Dose (Slice {slice_idx})')\n",
    "        axes[i, 2].axis('off')\n",
    "\n",
    "        if ground_truth is not None:\n",
    "            axes[i, 3].imshow(ground_truth[slice_idx, :, :], cmap='hot')\n",
    "            axes[i, 3].set_title(f'Ground Truth Dose (Slice {slice_idx})')\n",
    "            axes[i, 3].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        print(f\"Saved visualization to {save_path}\")\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# Test Model\n",
    "# ----------------------\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def denormalize_dose(normalized_dose, min_dose, max_dose):\n",
    "    \"\"\"\n",
    "    Denormalize the predicted dose back to original scale.\n",
    "    \n",
    "    Args:\n",
    "        normalized_dose (torch.Tensor): Normalized dose prediction\n",
    "        min_dose (float): Minimum dose value\n",
    "        max_dose (float): Maximum dose value\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Denormalized dose\n",
    "    \"\"\"\n",
    "    return normalized_dose * (max_dose - min_dose) + min_dose\n",
    "\n",
    "def evaluate_model(model, test_dataset, metadata_dict, device='cuda'):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test dataset.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): Trained model\n",
    "        test_dataset (Dataset): Test dataset\n",
    "        metadata_dict (dict): Dictionary containing dose metadata\n",
    "        device (str): Computing device\n",
    "    \n",
    "    Returns:\n",
    "        dict: Evaluation metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    mae_list = []\n",
    "    mse_list = []\n",
    "    \n",
    "    results = {\n",
    "        'patient_ids': [],\n",
    "        'mae_values': [],\n",
    "        'mse_values': []\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in range(len(test_dataset)):\n",
    "            # Get input data\n",
    "            input_tensor = test_dataset[idx]\n",
    "            \n",
    "            # Prepare input\n",
    "            input_tensor = input_tensor.unsqueeze(0).to(device)\n",
    "            \n",
    "            # Get filename for metadata lookup\n",
    "            filename = test_dataset.ct_paths[idx].split('/')[-1]\n",
    "            \n",
    "            # Get dose metadata\n",
    "            min_dose = metadata_dict[filename]['MinDose']\n",
    "            max_dose = metadata_dict[filename]['DynamicMaxDose']\n",
    "            \n",
    "            # Predict\n",
    "            prediction = model(input_tensor)\n",
    "            \n",
    "            # Denormalize prediction and ground truth\n",
    "            pred_dose = prediction.squeeze(0).cpu()\n",
    "            true_dose = torch.tensor(np.load(test_dataset.dose_paths[idx]))\n",
    "            \n",
    "            # Denormalize both prediction and ground truth\n",
    "            pred_dose_denorm = denormalize_dose(pred_dose, min_dose, max_dose)\n",
    "            true_dose_denorm = denormalize_dose(torch.tensor(true_dose), min_dose, max_dose)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mae = torch.mean(torch.abs(pred_dose_denorm - true_dose_denorm)).item()\n",
    "            mse = torch.mean((pred_dose_denorm - true_dose_denorm)**2).item()\n",
    "            \n",
    "            mae_list.append(mae)\n",
    "            mse_list.append(mse)\n",
    "            \n",
    "            # Store results\n",
    "            results['patient_ids'].append(filename)\n",
    "            results['mae_values'].append(mae)\n",
    "            results['mse_values'].append(mse)\n",
    "            \n",
    "            # Optional: Visualization of dose distribution\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.subplot(131)\n",
    "            plt.title('Predicted Dose')\n",
    "            plt.imshow(pred_dose_denorm.numpy()[pred_dose_denorm.shape[0]//2], cmap='hot')\n",
    "            plt.colorbar()\n",
    "            \n",
    "            plt.subplot(132)\n",
    "            plt.title('Ground Truth Dose')\n",
    "            plt.imshow(true_dose_denorm.numpy()[true_dose_denorm.shape[0]//2], cmap='hot')\n",
    "            plt.colorbar()\n",
    "            \n",
    "            plt.subplot(133)\n",
    "            plt.title('Difference')\n",
    "            plt.imshow(np.abs(pred_dose_denorm.numpy()[pred_dose_denorm.shape[0]//2] - \n",
    "                               true_dose_denorm.numpy()[true_dose_denorm.shape[0]//2]), cmap='cool')\n",
    "            plt.colorbar()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'dose_comparison_{filename}.png')\n",
    "            plt.close()\n",
    "    \n",
    "    # Compute overall metrics\n",
    "    overall_metrics = {\n",
    "        'mean_mae': np.mean(mae_list),\n",
    "        'std_mae': np.std(mae_list),\n",
    "        'mean_mse': np.mean(mse_list),\n",
    "        'std_mse': np.std(mse_list)\n",
    "    }\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv('test_results.csv', index=False)\n",
    "    \n",
    "    # Save overall metrics\n",
    "    with open('overall_metrics.txt', 'w') as f:\n",
    "        for key, value in overall_metrics.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "    \n",
    "    return overall_metrics\n",
    "\n",
    "def main():\n",
    "    # Load metadata\n",
    "    metadata_df = pd.read_csv('metadata.csv')\n",
    "    metadata_dict = {row['FileName']: {\n",
    "        'MinDose': row['MinDose'], \n",
    "        'DynamicMaxDose': row['DynamicMaxDose']\n",
    "    } for _, row in metadata_df.iterrows()}\n",
    "    \n",
    "    # Prepare test dataset\n",
    "    test_dataset = DosePredictionDataset(\n",
    "        ct_paths=[...],  # List of CT file paths\n",
    "        structure_paths=[...],  # List of structure file paths\n",
    "        dose_paths=[...],  # List of dose file paths\n",
    "        csv_path='metadata.csv',\n",
    "        mode='test',\n",
    "        normalize=True\n",
    "    )\n",
    "    \n",
    "    # Load trained model\n",
    "    model = Cascade3DUNet(...)  # Your model architecture\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    \n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics = evaluate_model(\n",
    "        model, \n",
    "        test_dataset, \n",
    "        metadata_dict, \n",
    "        device\n",
    "    )\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"Overall Test Metrics:\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# ----------------------\n",
    "# Convert Model Output to DICOM\n",
    "# ----------------------\n",
    "def save_dicom(pred_rd, dicom_template_path, output_dicom_path):\n",
    "    dicom_template = pydicom.dcmread(dicom_template_path)\n",
    "    pred_rd = pred_rd.cpu().numpy().squeeze()  # Convert to numpy array\n",
    "    dicom_template.PixelData = pred_rd.astype(np.float32).tobytes()\n",
    "    dicom_template.DoseGridScaling = np.max(pred_rd) / np.max(dicom_template.pixel_array)\n",
    "    dicom_template.RescaleSlope = 1.0\n",
    "    dicom_template.RescaleIntercept = 0.0\n",
    "    dicom_template.SOPInstanceUID = f\"1.2.826.0.1.3680043.10.511.{uuid.uuid4().int}\"\n",
    "    dicom_template.StudyInstanceUID = f\"1.2.826.0.1.3680043.10.511.{uuid.uuid4().int}\"\n",
    "    dicom_template.SeriesInstanceUID = f\"1.2.826.0.1.3680043.10.511.{uuid.uuid4().int}\"\n",
    "    dicom_template.save_as(output_dicom_path)\n",
    "\n",
    "# Example Usage\n",
    "dicom_template_path = \"original_dose.dcm\"\n",
    "output_dicom_path = \"predicted_dose.dcm\"\n",
    "save_dicom(pred_rd, dicom_template_path, output_dicom_path)\n",
    "\n",
    "# ----------------------\n",
    "# Visualize Dose Map\n",
    "# ----------------------\n",
    "dicom_new = pydicom.dcmread(output_dicom_path)\n",
    "dose_array = dicom_new.pixel_array\n",
    "plt.imshow(dose_array[dose_array.shape[0] // 2], cmap=\"jet\")\n",
    "plt.colorbar(label=\"Dose\")\n",
    "plt.title(\"Predicted Dose Distribution\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
